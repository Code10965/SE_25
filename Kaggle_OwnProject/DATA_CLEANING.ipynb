{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "still-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import scipy.stats as scs\n",
    "from scipy import stats\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-mentor",
   "metadata": {},
   "source": [
    "# Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relevant-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = 'datasets/aug_train.csv'\n",
    "train= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "headed-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_raw_data():\n",
    "    display(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protective-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates head of table of raw data \n",
    "#show_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-scanner",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recorded-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "golden-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_clean.csv'\n",
    "train_clean.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "neural-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'datasets/train_clean.csv'\n",
    "train_clean= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extraordinary-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 1:\n",
    "# Stripping 'city_' from values of column 'city'\n",
    "train_clean['city'] = train_clean['city'].map(lambda x: x.lstrip('city_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "loving-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 2:\n",
    "# Creating a new variable by rounding city_development_index\n",
    "train_clean['cdi_round'] = train_clean['city_development_index'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extra-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 3:\n",
    "# Replacing NaN-values by the value 'no info' for nominally scaled variables\n",
    "train_clean[['gender']] = train_clean[['gender']].replace(np.nan, 'no info', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "manufactured-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 6:\n",
    "# Replacing NaN-values by the value 'no info' for nominally scaled variables\n",
    "\n",
    "train_clean[['major_discipline']] = train_clean[['major_discipline']].replace(np.nan, 'no info', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "veterinary-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 8:\n",
    "# Creating new variable\n",
    "\n",
    "train_clean['experience_group'] = train_clean['experience']\n",
    "train_clean['experience_group'] = train_clean['experience_group'].replace((\"<1\",\"1\",\"2\",\"3\",\"4\"),\"<5\")\n",
    "train_clean['experience_group'] = train_clean['experience_group'].replace((\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"),\"5-10\")\n",
    "train_clean['experience_group'] = train_clean['experience_group'].replace((\"11\",\"12\",\"13\",\"14\",\"15\"),\"11-15\")\n",
    "train_clean['experience_group'] = train_clean['experience_group'].replace((\"16\",\"17\",\"18\",\"19\",\"20\"),\"16-20\")\n",
    "train_clean['experience_group'] = train_clean['experience_group'].replace(\">20\",\">20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "monetary-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 9 - - raus\n",
    "# Replacing NaN-values by the value 'no info' for nominally scaled variables\n",
    "train_clean[['company_size']] = train_clean[['company_size']].replace(np.nan, 'no info', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "upper-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 10:\n",
    "# Replacing NaN-values by the value 'no info' for nominally scaled variables\n",
    "train_clean[['company_type']] = train_clean[['company_type']].replace(np.nan, 'no info', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "available-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 12\n",
    "# Creating new variables\n",
    "\n",
    "train_clean['training_ten_hours'] = (np.floor(train_clean['training_hours']/40)*40).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "facial-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep 4 / Prep 5 / Prep 7 / Prep 9/ Prep 11:\n",
    "# Deleting rows with NaN-values for ordinally scaled variables\n",
    "train_clean = train_clean.dropna()\n",
    "  \n",
    "# To reset the indices \n",
    "train_clean = train_clean.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elect-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_clean.csv'\n",
    "train_clean.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greenhouse-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clean_data():\n",
    "    print(\"Number of rows and columns in data set:\", train_clean.shape)\n",
    "    display(train_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "devoted-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates head of table of clean data \n",
    "#show_clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-worth",
   "metadata": {},
   "source": [
    "### Transfroming data: train_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infrared-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_next = train_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "welcome-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_clean_next.csv'\n",
    "train_clean_next.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "purple-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'datasets/train_clean_next.csv'\n",
    "train_clean_next= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "charming-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_next['gender_original'] = train_clean_next['gender']\n",
    "train_clean_next['relevent_experience_original'] = train_clean_next['relevent_experience']\n",
    "train_clean_next['enrolled_university_original'] = train_clean_next['enrolled_university']\n",
    "train_clean_next['education_level_original'] = train_clean_next['education_level']\n",
    "train_clean_next['major_discipline_original'] = train_clean_next['major_discipline']\n",
    "train_clean_next['experience_group_original'] = train_clean_next['experience_group']\n",
    "train_clean_next['company_size_original'] = train_clean_next['company_size']\n",
    "train_clean_next['company_type_original'] = train_clean_next['company_type']\n",
    "train_clean_next['last_new_job_original'] = train_clean_next['last_new_job']\n",
    "train_clean_next = pd.get_dummies(train_clean_next, \n",
    "                          columns=[\n",
    "                                'gender',\n",
    "                                'relevent_experience',\n",
    "                                'enrolled_university',\n",
    "                                'education_level',\n",
    "                                'major_discipline',\n",
    "                                'experience_group',\n",
    "                                'company_size',\n",
    "                                'company_type',\n",
    "                                'last_new_job'\n",
    "                                  ], \n",
    "                          prefix=[\n",
    "                                'gender',\n",
    "                                'relevent_experience',\n",
    "                                'enrolled_university',\n",
    "                                'education_level',\n",
    "                                'major_discipline',\n",
    "                                'experience_group',\n",
    "                                'company_size',\n",
    "                                'company_type',\n",
    "                                'last_new_job'\n",
    "                                  ])\n",
    "\n",
    "train_clean_next=train_clean_next.rename(columns={\n",
    "\n",
    "                                \"gender_Female\": \"Female\",\n",
    "                                \"gender_Male\": \"Male\",\n",
    "                                \"gender_Other\": \"Gender_other\",\n",
    "                                \"gender_no info\": \"Gender_no_info\",\n",
    "    \n",
    "                                \"relevent_experience_No relevent experience\" :\"No_experience_in_data_science\",\n",
    "                                \"relevent_experience_Has relevent experience\" :\"Experience_in_data_science\",\n",
    "    \n",
    "                                \"enrolled_university_no_enrollment\" :\"No_course\",\n",
    "                                \"enrolled_university_Part time course\" :\"Part_time_course\",\n",
    "                                \"enrolled_university_Full time course\" :\"Full_time_course\",\n",
    "    \n",
    "                                \"education_level_Primary School\" : \"Primary_School\", \n",
    "                                \"education_level_High School\" : \"High_School\",\n",
    "                                \"education_level_Masters\" : \"Masters\",\n",
    "                                \"education_level_Graduate\" : \"Graduate\",\n",
    "                                \"education_level_Phd\" : \"Phd\",\n",
    "    \n",
    "                                \"major_discipline_Arts\" : \"Major_in_Arts\",\n",
    "                                \"major_discipline_Business Degree\" : \"Major_in_Business_Degree\",\n",
    "                                \"major_discipline_Humanities\" : \"Major_in_Humanities\",\n",
    "                                \"major_discipline_STEM\" : \"Major_in_STEM\",\n",
    "                                \"major_discipline_Other\" : \"Major_in_Other\",\n",
    "                                \"major_discipline_No Major\" : \"No_Major\",\n",
    "                                \"major_discipline_no info\" : \"Major_no_info\",\n",
    "    \n",
    "                                \"company_type_Early Stage Startup\" : \"Currently_in_Early_Stage_Startup\",\n",
    "                                \"company_type_Funded Startup\" : \"Currently_in_Funded_Startup\",\n",
    "                                \"company_type_Public Sector\" : \"Currently_in_Public_Sector\",\n",
    "                                \"company_type_Pvt Ltd\" : \"Currently_in_Pvt_Ltd\",\n",
    "                                \"company_type_NGO\" : \"Currently_in_NGO\",\n",
    "                                \"company_type_Other\" : \"Currently_in_other_company_type\",\n",
    "                                \"company_type_no info\" : \"Company_type_no_info\",\n",
    "    \n",
    "                                \"last_new_job_never\" : \"Last_new_job_never\",\n",
    "                                \"last_new_job_1\" : \"1_year_between_previous_and_current_job\",\n",
    "                                \"last_new_job_2\" : \"2_years_between_previous_and_current_job\",\n",
    "                                \"last_new_job_3\" : \"3_years_between_previous_and_current_job\",\n",
    "                                \"last_new_job_4\" : \"4_years_between_previous_and_current_job\",\n",
    "                                \"last_new_job_>4\" : \"More_than_4_years_between_previous_and_current_job\",\n",
    "    \n",
    "                                \"experience_group_<5\" : \"Work_experience_up_to_5_y\",\n",
    "                                \"experience_group_5-10\" : \"Work_experience_5_to_10_y\",\n",
    "                                \"experience_group_11-15\" : \"Work_experience_11_to_15_y\",\n",
    "                                \"experience_group_16-20\" : \"Work_experience_16_to_20_y\",\n",
    "                                \"experience_group_>20\" : \"Work_experience_more_than_20_y\",\n",
    "    \n",
    "                                \"company_size_<10\" : \"Company_size_up_to_10\",\n",
    "                                \"company_size_10/49\" : \"Company_size_10_to_49\",\n",
    "                                \"company_size_50-99\" : \"Company_size_50_to_99\",\n",
    "                                \"company_size_100-500\" : \"Company_size_100_to_500\",\n",
    "                                \"company_size_500-999\" : \"Company_size_500_to_999\",\n",
    "                                \"company_size_1000-4999\" : \"Company_size_1000_to_4999\",\n",
    "                                \"company_size_5000-9999\" : \"Company_size_5000_to_9999\",\n",
    "                                \"company_size_10000+\" : \"Company_size_more_than_10000\",\n",
    "                                \"company_size_no info\" : \"Company_size_no_info\"      \n",
    "                                }, inplace = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "twelve-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = train_clean_next.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "other-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_cleaned.csv'\n",
    "train_cleaned.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-wings",
   "metadata": {},
   "source": [
    "### Transfroming data: train_model (for XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "disturbed-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_drop = train_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "infrared-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_cleaned_drop.csv'\n",
    "train_cleaned_drop.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sexual-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'datasets/train_cleaned_drop.csv'\n",
    "train_cleaned_drop= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "flush-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Gender_no_info','Company_type_no_info','Major_no_info','enrollee_id', 'experience','gender_original','relevent_experience_original','enrolled_university_original','education_level_original',\n",
    "           'major_discipline_original','experience_group_original','company_size_original',\n",
    "           'company_type_original','last_new_job_original']\n",
    "train_cleaned_drop.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "personalized-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = train_cleaned_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "instrumental-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_model.csv'\n",
    "train_model.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "official-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18014, 51)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-robin",
   "metadata": {},
   "source": [
    "### Transfroming data: train_binary (for SHAP values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "moving-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_next = train_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "advised-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_model_next.csv'\n",
    "train_model_next.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "occupational-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'datasets/train_model_next.csv'\n",
    "train_model_next= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "local-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_next['city_split'] = train_model_next['city']\n",
    "train_model_next['city_split'] = '0'\n",
    "train_model_next.loc[train_model_next['city'] == 21, 'city_split'] = '1'\n",
    "\n",
    "train_model_next['cdi_round'] = train_model_next['city_development_index'].round(1)\n",
    "train_model_next['cdi_split'] = train_model_next['cdi_round'].map({\n",
    "                                    0.0: 1,\n",
    "                                    0.1: 1,\n",
    "                                    0.2: 1,\n",
    "                                    0.3: 1,\n",
    "                                    0.4: 1,\n",
    "                                    0.5: 1,\n",
    "                                    0.6: 1,\n",
    "                                    0.7: 0,\n",
    "                                    0.8: 0,\n",
    "                                    0.9: 0,\n",
    "                                    1.0: 0\n",
    "                                    })\n",
    "train_model_next['training_week'] = (np.floor(train_model_next['training_hours']/40)*40).astype(int)\n",
    "train_model_next['training_split']= train_model_next['training_week'].map({\n",
    "                                0 : 0,\n",
    "                                40 : 0,\n",
    "                                80 : 1,\n",
    "                                120 : 1,\n",
    "                                160 : 1,\n",
    "                                200 : 1,\n",
    "                                240 : 1,\n",
    "                                280 : 1,\n",
    "                                320 : 1,\n",
    "                                360 : 1,\n",
    "                                400 : 1\n",
    "                                    }).astype(int)\n",
    "\n",
    "\n",
    "train_model_next = train_model_next.drop([\n",
    "                                'city',\n",
    "                                'city_development_index',\n",
    "                                'cdi_round',\n",
    "                                'training_hours',\n",
    "                                'training_week'\n",
    "                                ], axis=1)\n",
    "\n",
    "train_model_next = pd.get_dummies(train_model_next, \n",
    "                          columns=[\n",
    "                                'city_split',\n",
    "                                'cdi_split',\n",
    "                                'training_split'\n",
    "                                  ], \n",
    "                          prefix=[\n",
    "                                'city_split',\n",
    "                                'cdi_split',\n",
    "                                'training_split'\n",
    "                                  ])\n",
    "\n",
    "train_model_next=train_model_next.rename(columns={ \n",
    "                                \"city_split_0\" : \"City_is_not_City21\",\n",
    "                                \"city_split_1\" : \"City_is_City21\",\n",
    "                                \"cdi_split_0\" : \"City_development_index_more_than_0.6\",\n",
    "                                \"cdi_split_1\" : \"City_development_index_up_to_0.6\",\n",
    "                                \"training_split_0\" :\"Training_up_to_40_hours\",\n",
    "                                \"training_split_1\" :\"Training_more_than_40_hours\"\n",
    "                                }, inplace = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acoustic-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['training_ten_hours']\n",
    "train_model_next.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hindu-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary = train_model_next.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "advisory-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_binary.csv'\n",
    "train_binary.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-story",
   "metadata": {},
   "source": [
    "### Transfroming data: train_chi (for Chi2-tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "political-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_next = train_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "conservative-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_cleaned_next.csv'\n",
    "train_cleaned_next.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "tender-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'datasets/train_cleaned_next.csv'\n",
    "train_cleaned_next= pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "integral-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating binary variables for city_development_index:\n",
    "train_cleaned_next['cdi_round'] = train_cleaned_next['city_development_index'].round(1)\n",
    "train_cleaned_next['cdi_split']= train_cleaned_next['cdi_round'].map({\n",
    "                                    0.0: 1,\n",
    "                                    0.1: 1,\n",
    "                                    0.2: 1,\n",
    "                                    0.3: 1,\n",
    "                                    0.4: 1,\n",
    "                                    0.5: 1,\n",
    "                                    0.6: 1,\n",
    "                                    0.7: 0,\n",
    "                                    0.8: 0,\n",
    "                                    0.9: 0,\n",
    "                                    1.0: 0\n",
    "                                    }).astype(int)\n",
    "\n",
    "# Creating binary variables for city:\n",
    "train_cleaned_next['city_split'] = train_cleaned_next['city']\n",
    "train_cleaned_next['city_split'] = '0'\n",
    "train_cleaned_next.loc[train_cleaned_next['city'] == 21, 'city_split'] = '1'\n",
    "\n",
    "# Creating binary variables for experience_split:\n",
    "train_cleaned_next['experience_split_20']= train_cleaned_next['experience_group_original'].map({\n",
    "                                \"<5\": 0,\n",
    "                                \"5-10\": 0,\n",
    "                                \"11-15\": 0,\n",
    "                                \"16-20\": 0,\n",
    "                                \">20\": 1\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for experience_split:\n",
    "train_cleaned_next['experience_split_5']= train_cleaned_next['experience_group_original'].map({\n",
    "                                \"<5\": 1,\n",
    "                                \"5-10\": 0,\n",
    "                                \"11-15\": 0,\n",
    "                                \"16-20\": 0,\n",
    "                                \">20\": 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for relevant_experience_split:\n",
    "train_cleaned_next['relevant_experience_split']= train_cleaned_next['relevent_experience_original'].map({\n",
    "                                    \"No relevent experience\": 1,\n",
    "                                    \"Has relevent experience\": 0\n",
    "                                    }).astype(int)\n",
    "\n",
    "# Creating binary variables for last_job_1year:\n",
    "train_cleaned_next['last_new_job_split']= train_cleaned_next['last_new_job_original'].map({\n",
    "                                \"never\" : 1,\n",
    "                                \"1\" : 0,\n",
    "                                \"2\" : 0,\n",
    "                                \"3\" : 0,\n",
    "                                \"4\" : 0,\n",
    "                                \">4\" : 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for company_type_split:\n",
    "train_cleaned_next['company_type_split']= train_cleaned_next['company_type_original'].map({\n",
    "                                \"Early Stage Startup\": 0,\n",
    "                                \"Funded Startup\": 0,\n",
    "                                \"Public Sector\": 0,\n",
    "                                \"Pvt Ltd\": 1,\n",
    "                                \"NGO\": 0,\n",
    "                                \"Other\": 0,\n",
    "                                \"no info\" : 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for enrolled_university_split:\n",
    "train_cleaned_next['enrolled_university_split']= train_cleaned_next['enrolled_university_original'].map({\n",
    "                                \"no_enrollment\": 1,\n",
    "                                \"Part time course\": 0,\n",
    "                                \"Full time course\": 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for education_level_split:\n",
    "train_cleaned_next['education_level_split_g']= train_cleaned_next['education_level_original'].map({\n",
    "                                \"Primary School\": 0,\n",
    "                                \"High School\": 0,\n",
    "                                \"Masters\": 0,\n",
    "                                \"Graduate\": 1,\n",
    "                                \"Phd\": 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for education_level_split:\n",
    "train_cleaned_next['education_level_split_h']= train_cleaned_next['education_level_original'].map({\n",
    "                                \"Primary School\": 0,\n",
    "                                \"High School\": 1,\n",
    "                                \"Masters\": 0,\n",
    "                                \"Graduate\": 0,\n",
    "                                \"Phd\": 0\n",
    "                                })\n",
    "\n",
    "# Creating binary variables for company_size_split:\n",
    "train_cleaned_next['company_size_split']= train_cleaned_next['company_size_original'].map({\n",
    "                               \"<10\" : 0,\n",
    "                                \"10/49\" : 0,\n",
    "                                \"50-99\" : 0, \n",
    "                                \"100-500\" : 0,\n",
    "                                \"500-999\" : 0,\n",
    "                                \"1000-4999\" : 0,\n",
    "                                \"5000-9999\" : 0,\n",
    "                                \"5000_to_9999\": 0,\n",
    "                                \"10000+\" : 0,\n",
    "                                \"no info\" : 1 \n",
    "                                })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "graphic-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chi = train_cleaned_next.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "advance-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_chi.csv'\n",
    "train_chi.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-spirituality",
   "metadata": {},
   "source": [
    "### Transfroming data: train_graph (for cat plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "grateful-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "rental-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph['cdi_round'] = train_graph['city_development_index'].round(1)\n",
    "train_graph['cdi_split'] = train_graph['cdi_round'].map({\n",
    "                                    0.0: 0,\n",
    "                                    0.1: 0,\n",
    "                                    0.2: 0,\n",
    "                                    0.3: 0,\n",
    "                                    0.4: 0,\n",
    "                                    0.5: 0,\n",
    "                                    0.6: 0,\n",
    "                                    0.7: 1,\n",
    "                                    0.8: 1,\n",
    "                                    0.9: 1,\n",
    "                                    1.0: 1\n",
    "                                    })\n",
    "\n",
    "train_graph['City_split'] = train_graph['city']\n",
    "\n",
    "train_graph.loc[train_graph['City_split'] != \"city_21\", 'City_split'] = \"not_city_21\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "naked-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph['Company Type']=train_graph['company_type']\n",
    "train_graph['Highest Education']= train_graph['education_level']\n",
    "train_graph['City_Development_Index']=train_graph['cdi_split']\n",
    "train_graph['City_Development_Index']=train_graph['City_Development_Index'].replace(1,\"> 0.6\")\n",
    "train_graph['City_Development_Index']=train_graph['City_Development_Index'].replace(0,\"<= 0.6\")\n",
    "train_graph['Company Type']=train_graph['Company Type'].replace((\"Early Stage Startup\",\"Funded Startup\",\"Public Sector\", \"NGO\", \"Other\", \"no info\"), \"NOT employed in a Pvt Ltd company\")\n",
    "train_graph['Company Type']=train_graph['Company Type'].replace(\"Pvt Ltd\", \"Employed in a Pvt Ltd company\")\n",
    "train_graph['Highest Education']=train_graph['Highest Education'].replace( (\"Primary School\",\"High School\",\"Masters\", \"Phd\"),\"NOT Graduate\")\n",
    "train_graph['Highest Education']=train_graph['Highest Education'].replace(\"Graduate\",\"Graduate\")\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "civic-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph['DataScience_Experience']=train_graph['relevent_experience']\n",
    "train_graph['Working_Experience']= train_graph['experience']\n",
    "train_graph['City_Development_Index']=train_graph['cdi_split']\n",
    "train_graph['City_Development_Index']=train_graph['City_Development_Index'].replace(1,\"> 0.6\")\n",
    "train_graph['City_Development_Index']=train_graph['City_Development_Index'].replace(0,\"<= 0.6\")\n",
    "train_graph['DataScience_Experience']=train_graph['DataScience_Experience'].replace(\"No relevent experience\",\"Novice in data science\")\n",
    "train_graph['DataScience_Experience']=train_graph['DataScience_Experience'].replace(\"Has relevent experience\",\"Experienced in data science\")\n",
    "train_graph['Working_Experience']=train_graph['Working_Experience'].replace((\"<1\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"),\"Under 20 years worked\")\n",
    "train_graph['Working_Experience']=train_graph['Working_Experience'].replace(\">20\",\"Over 20 years worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "focal-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_graph = train_graph.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rocky-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/train_cat_graph.csv'\n",
    "train_cat_graph.to_csv(path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 201,
   "position": {
    "height": "223px",
    "left": "789px",
    "right": "20px",
    "top": "130px",
    "width": "563px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
